{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STraDA assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_version = \"0.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricsCount(df, dataset, zoom=None):\n",
    "    #hue_palette = sns.color_palette([\"#6acc64\", \"#ee854a\", \"#d65f5f\", \"#797979\"])  # Muted\n",
    "    hue_palette = sns.color_palette([\"#1ac938\", \"#ff7c00\", \"#e8000b\", \"#a3a3a3\"])  # Bright\n",
    "    hue_order = [\"TP\", \"FN\", \"FP\", \"-\"]\n",
    "\n",
    "    g = sns.catplot(data=df[df.dataset == dataset],\n",
    "                kind=\"count\",\n",
    "                x=\"source\",\n",
    "                hue=\"status\",\n",
    "                col=\"type\",\n",
    "                hue_order=hue_order,\n",
    "                palette=hue_palette\n",
    "               )\n",
    "    plt.subplots_adjust(top=0.8)\n",
    "    g.fig.suptitle('Fusions callers comparison on ' + dataset)\n",
    "    if zoom is not None:\n",
    "        axes = g.axes\n",
    "        axes[0,0].set_ylim(0, zoom)\n",
    "        axes[0,1].set_ylim(0, zoom)\n",
    "    plt.show()\n",
    "\n",
    "def perfTable(df):\n",
    "    col_order = ['source', 'TP', 'FP', 'FN', '-', 'precision', 'recall']\n",
    "    rows = list()\n",
    "    for source in sorted(set(df.source)):\n",
    "        curr_row = {\"source\": source, \"TP\": 0, \"FP\": 0, \"FN\": 0, \"-\": 0}\n",
    "        for status, count in df[df.source == source][\"status\"].value_counts().items():\n",
    "            curr_row[status] = count\n",
    "        curr_row[\"precision\"] = \"{:.4f}\".format(curr_row[\"TP\"] / (curr_row[\"TP\"] + curr_row[\"FP\"]))\n",
    "        curr_row[\"recall\"] = \"{:.4f}\".format(curr_row[\"TP\"] / (curr_row[\"TP\"] + curr_row[\"FN\"]))\n",
    "        rows.append(curr_row)\n",
    "    display(pd.DataFrame(rows)[col_order])\n",
    "\n",
    "def perfTable(df):\n",
    "    # Performance\n",
    "    col_order = ['source', 'TP', 'FP', 'FN', '-', 'precision', 'recall']\n",
    "    rows = list()\n",
    "    res_by_src = {}\n",
    "    for source in sorted(set(df.source)):\n",
    "        curr_row = {\"source\": source, \"TP\": 0, \"FP\": 0, \"FN\": 0, \"-\": 0}\n",
    "        for status, count in df[df.source == source][\"status\"].value_counts().items():\n",
    "            curr_row[status] = count\n",
    "        curr_row[\"precision\"] = \"{:.4f}\".format(curr_row[\"TP\"] / (curr_row[\"TP\"] + curr_row[\"FP\"]))\n",
    "        curr_row[\"recall\"] = \"{:.4f}\".format(curr_row[\"TP\"] / (curr_row[\"TP\"] + curr_row[\"FN\"]))\n",
    "        rows.append(curr_row)\n",
    "        res_by_src[source] = curr_row\n",
    "    display(pd.DataFrame(rows)[col_order])\n",
    "    # Significance\n",
    "    sig_rows = list()\n",
    "    for curr_row in rows:\n",
    "        curr_source = curr_row[\"source\"]\n",
    "        sig_row = {\"source\": curr_source}\n",
    "        for cmp_source in sorted(set(df.source)):\n",
    "            if curr_source == cmp_source:\n",
    "                sig_row[\"(prec) \" + cmp_source] = \"\"\n",
    "                sig_row[\"(rec) \" + cmp_source] = \"\"\n",
    "            else:\n",
    "                curr_res = res_by_src[curr_source]\n",
    "                cmp_res = res_by_src[cmp_source]\n",
    "                odds_ratio, p_value = stats.fisher_exact([\n",
    "                    [curr_res[\"TP\"], cmp_res[\"TP\"]],\n",
    "                    [curr_res[\"TP\"] + curr_res[\"FP\"], cmp_res[\"TP\"] + cmp_res[\"FP\"]]\n",
    "                ])\n",
    "                sig_row[\"(prec) \" + cmp_source] = p_value\n",
    "                odds_ratio, p_value = stats.fisher_exact([\n",
    "                    [curr_res[\"TP\"], cmp_res[\"TP\"]],\n",
    "                    [curr_res[\"TP\"] + curr_res[\"FN\"], cmp_res[\"TP\"] + cmp_res[\"FN\"]]\n",
    "                ])\n",
    "                sig_row[\"(rec) \" + cmp_source] = p_value\n",
    "        sig_rows.append(sig_row)\n",
    "    col_order = [\"source\"] + [\"(prec) \" + src for src in sorted(set(df.source))]\n",
    "    display(pd.DataFrame(sig_rows)[col_order])\n",
    "    col_order = [\"source\"] + [\"(rec) \" + src for src in sorted(set(df.source))]\n",
    "    display(pd.DataFrame(sig_rows)[col_order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genes = pd.read_csv(curr_version + \"/results_details_genes.tsv\", sep=\"\\t\")\n",
    "df_genes[\"type\"] = df_genes[\"dataset\"].apply(lambda x: \"genes\")\n",
    "df_breakpoints = pd.read_csv(curr_version + \"/results_details_breakpoints.tsv\", sep=\"\\t\")\n",
    "df_breakpoints[\"type\"] = df_breakpoints[\"dataset\"].apply(lambda x: \"breakpoints\")\n",
    "df = pd.concat([df_genes, df_breakpoints])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Litterature dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1- Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2- Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dataset = \"Heyer_2019\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsCount(df, curr_dataset)\n",
    "metricsCount(df, curr_dataset, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in sorted(set(df.type)):\n",
    "    print(data_type.capitalize())\n",
    "    perfTable(df[(df.dataset == curr_dataset) & (df.type == data_type)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Simulated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1- Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2- Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dataset = \"simulated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsCount(df, curr_dataset)\n",
    "metricsCount(df, curr_dataset, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in sorted(set(df.type)):\n",
    "    print(data_type.capitalize())\n",
    "    perfTable(df[(df.dataset == curr_dataset) & (df.type == data_type)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1- Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2- Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dataset = \"Tembe_2014\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsCount(df, curr_dataset)\n",
    "metricsCount(df, curr_dataset, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in sorted(set(df.type)):\n",
    "    print(data_type.capitalize())\n",
    "    perfTable(df[(df.dataset == curr_dataset) & (df.type == data_type)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
